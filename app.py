import streamlit as st
from helper import load_pt_model, get_image_download_buffer, draw_bounding_boxes, crop_images
from keras.api.models import load_model as load_h5_model
from pathlib import Path
import numpy as np
import PIL
import settings
import zipfile
import io
from torch.nn.functional import interpolate
import csv
from typing import List, Dict, Any
from PIL import Image
import base64
import time
from tqdm import tqdm 
from streamlit.components.v1 import html as html_component
import torch  # Agregar esta importaci√≥n
from transformers import ViTForImageClassification, ViTImageProcessor
import matplotlib.pyplot as plt

# Constantes globales de la aplicaci√≥n
IOU_THRES = 0.5  # Umbral de IoU para la supresi√≥n de no m√°ximos
CLASSES_NAME = ['both', 'infection', 'ischaemia', 'none']  # Nombres de clases en ingl√©s
CLASSES_NAME_ES = ['INFECCI√ìN E ISQUEMIA', 'INFECCI√ìN', 'ISQUEMIA', 'SANO']  # Nombres de clases en espa√±ol
# Decorador para cachear el modelo de detecci√≥n y evitar recargas innecesarias
@st.cache_resource
def load_models():
    """
    Carga todos los modelos necesarios y los cachea para evitar recargas innecesarias.
    """
    try:
        det_model = load_pt_model(Path(settings.DETECTION_MODEL))
        model_path = 'weights/vit3con32'
        processor = ViTImageProcessor.from_pretrained(model_path)
        clf_model = ViTForImageClassification.from_pretrained(
            model_path,
            num_labels=len(CLASSES_NAME),
            ignore_mismatched_sizes=True,
            output_attentions=True
        )
        return det_model, processor, clf_model
    except Exception as ex:
        st.error("No se pudo cargar los modelos. Verifique las rutas especificadas")
        st.error(ex)
        return None, None, None

def initialize_session() -> None:
    """
    Inicializa todas las variables necesarias en el estado de la sesi√≥n de Streamlit.
    Esta funci√≥n se ejecuta al inicio de la aplicaci√≥n y cada vez que se limpia la sesi√≥n.
    """
    # Lista para almacenar las im√°genes subidas por el usuario
    if 'uploaded_images' not in st.session_state:
        st.session_state.uploaded_images = []
    
    # Lista para almacenar las im√°genes procesadas por el modelo
    if 'processed_images' not in st.session_state:
        st.session_state.processed_images = []
    
    # Valor de confianza inicial para el modelo de detecci√≥n
    if 'confidence' not in st.session_state:
        st.session_state.confidence = 30
    
    # Clave √∫nica para el cargador de archivos, se incrementa para forzar su reinicializaci√≥n
    if 'uploader_key' not in st.session_state:
        st.session_state.uploader_key = 0

    # Modo de procesamiento (detecci√≥n o clasificaci√≥n directa)
    if 'use_detection' not in st.session_state:
        st.session_state.use_detection = True

    # Control de visualizaci√≥n de atenci√≥n (inicializado en True)
    if 'show_attention' not in st.session_state:
        st.session_state.show_attention = True

    # Diccionario para almacenar los resultados de clasificaci√≥n por imagen
    if 'classification_cache' not in st.session_state:
        st.session_state.classification_cache = {}

def clear_session() -> None:
    """
    Limpia el estado de la sesi√≥n, eliminando las im√°genes cargadas y procesadas.
    """
    if 'uploaded_images' in st.session_state:
        del st.session_state.uploaded_images
    
    if 'processed_images' in st.session_state:
        del st.session_state.processed_images
    
    if 'classification_cache' in st.session_state:
        del st.session_state.classification_cache
    
    # Resetear el estado de atenci√≥n cuando se limpia la sesi√≥n
    st.session_state.show_attention = True
    
    st.session_state.uploader_key += 1

def style_language_uploader():
    """
    Personaliza el estilo y texto del cargador de archivos en espa√±ol.
    Modifica el CSS para cambiar los textos del bot√≥n y las instrucciones.
    """
    # Diccionario con los textos en espa√±ol
    languages = {
        "es": {
            "button": "Cargar archivos",
            "instructions": "Arrastra y suelta archivos aqu√≠",
            "limits": "L√≠mite 5MB por archivo ‚Ä¢ JPG, JPEG, PNG",
        },
    }

    # CSS personalizado para el cargador de archivos
    hide_label = (
        """
        <style>
            /* Estilo para el bot√≥n principal */
            div[data-testid="stFileUploader"]>section[data-testid="stFileUploaderDropzone"]>button[data-testid="stBaseButton-secondary"] {
                width: 100%;
                visibility: hidden;
                position: relative;
            }
            
            /* Texto personalizado para el bot√≥n */
            div[data-testid="stFileUploader"]>section[data-testid="stFileUploaderDropzone"]>button[data-testid="stBaseButton-secondary"]::after {
                content: "BUTTON_TEXT";
                visibility: visible;
                display: block;
                position: absolute;
                width: 100%;
                top: 50%;
                left: 50%;
                transform: translate(-50%, -50%);
                font-family: inherit;
                font-size: inherit;
                color: inherit;
                background-color: inherit;
                border: inherit;
                border-radius: 8px;
                padding: 5px 10px;
            }
            
 /* Ajustar altura del contenedor del cargador */
            div[data-testid="stFileUploader"] section[data-testid="stFileUploaderDropzone"] {
                min-height: 100px !important;
                padding: 10px !important;
            }
            /* Estilo para las instrucciones */
            div[data-testid="stFileUploaderDropzoneInstructions"]>div>span {
                visibility: hidden;
            }
            div[data-testid="stFileUploaderDropzoneInstructions"]>div>span::after {
                content: "INSTRUCTIONS_TEXT";
                visibility: visible;
                display: block;
            }
            
            /* Estilo para los l√≠mites de archivo */
            div[data-testid="stFileUploaderDropzoneInstructions"]>div>small {
                visibility: hidden;
            }
            div[data-testid="stFileUploaderDropzoneInstructions"]>div>small::before {
                content: "FILE_LIMITS";
                visibility: visible;
                display: block;
            }
        </style>
        """
        .replace("BUTTON_TEXT", languages.get('es').get("button"))
        .replace("INSTRUCTIONS_TEXT", languages.get('es').get("instructions"))
        .replace("FILE_LIMITS", languages.get('es').get("limits"))
    )
    st.markdown(hide_label, unsafe_allow_html=True)

def write_csv(processed_images: List[Dict[str, Any]], classes_name: List[str]) -> str:
    """Genera un archivo CSV con las coordenadas de las cajas delimitadoras de las im√°genes procesadas.

    Args:
        processed_images (List[Dict[str, Any]]): Lista de diccionarios donde cada diccionario contiene el
            nombre de archivo, las cajas delimitadoras y las clasificaciones de una imagen procesada.
        classes_name (List[str]): Nombre perteneciente a cada clase. 

    Returns:
        str: El contenido del archivo CSV como una cadena de texto.
    """
    # Crear un archivo CSV en memoria para almacenar las coordenadas
    csv_buffer = io.StringIO()
    csv_writer = csv.writer(csv_buffer)
    
    # Escribir la cabecera del CSV seg√∫n el modo
    if st.session_state.detection_toggle:
        csv_writer.writerow(['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class'])
    else:
        csv_writer.writerow(['filename', 'class'])

    for img in processed_images:
        if st.session_state.detection_toggle:
            # Modo detecci√≥n: escribir coordenadas y clase para cada detecci√≥n
            for box, clf in zip(img['boxes'], img['classes']):
                xmin, ymin, xmax, ymax = [round(coord.item(), 2) for coord in box.xyxy[0]]
                csv_writer.writerow([img['filename'], xmin, ymin, xmax, ymax, classes_name[clf]])
        else:
            # Modo clasificaci√≥n: escribir solo el nombre del archivo y la clase
            # Obtener la clase directamente de la detecci√≥n
            class_name = img['detections'][0]['class']
            csv_writer.writerow([img['filename'], class_name])

    # Devolver el contenido del CSV como una cadena de texto
    return csv_buffer.getvalue()

def check_duplicates(uploaded_files: list):
    """
    Verifica si hay archivos duplicados en la lista de archivos subidos.
    
    Args:
        uploaded_files: Lista de archivos subidos por el usuario
        
    Returns:
        Lista con los nombres de los archivos duplicados
    """
    seen_files = set()  # Conjunto para almacenar nombres de archivos vistos
    duplicate_files = []  # Lista para almacenar nombres de archivos duplicados

    for file in uploaded_files:
        if file.name in seen_files:
            duplicate_files.append(file.name)
        else:
            seen_files.add(file.name)

    return duplicate_files

def process_images(det_model, clf_model, processor, confidence: float, iou_thres: float, classes_name: List[str]) -> None:
    """
    Procesa las im√°genes usando los modelos de detecci√≥n y clasificaci√≥n de manera m√°s eficiente.
    Siempre realiza tanto la detecci√≥n como la clasificaci√≥n, pero muestra los resultados seg√∫n el modo seleccionado.
    """
    # Limpiar el estado antes de procesar nuevas im√°genes
    st.session_state.processed_images = []
    
    # Asegurar que el estado de atenci√≥n est√© sincronizado con el modo de detecci√≥n
    if not st.session_state.detection_toggle:
        st.session_state.show_attention = True
    
    target_size = (224, 224)
    processed_results = []  # Lista temporal para almacenar resultados
    
    for image in st.session_state.uploaded_images:
        image_name = image.name
        uploaded_image = PIL.Image.open(image)
        
        # Verificar si ya tenemos los resultados de clasificaci√≥n en cach√©
        if image_name in st.session_state.classification_cache:
            cached_results = st.session_state.classification_cache[image_name]
            
            if st.session_state.detection_toggle:
                # Si estamos en modo detecci√≥n, usar los resultados de detecci√≥n
                detections = cached_results['detections']
                bboxes = cached_results['boxes']
                classes = cached_results['classes']
            else:
                # Si estamos en modo clasificaci√≥n directa, usar la clasificaci√≥n de la imagen completa
                detections = [cached_results['full_image_classification']]
                bboxes = []
                classes = [cached_results['full_image_class']]
        else:
            # Si no hay resultados en cach√©, procesar la imagen
            # Realizar la clasificaci√≥n de la imagen completa primero
            with torch.no_grad():
                resized_image = uploaded_image.resize(target_size)
                inputs = processor(images=[resized_image], return_tensors="pt")
                outputs = clf_model(**inputs)
                full_image_class = outputs.logits.argmax(-1).item()
                
                # Procesar atenciones para la imagen completa
                attentions = outputs.attentions[-1][0]
                full_image_attention = process_attention_maps(attentions, uploaded_image)
                
                full_image_detection = {
                    'original_image': uploaded_image,
                    'attention_map': full_image_attention,
                    'class': CLASSES_NAME_ES[full_image_class]
                }
            
            # Siempre realizar la detecci√≥n y clasificaci√≥n de las detecciones
            det_res = det_model.predict(uploaded_image, conf=confidence, iou=iou_thres)
            bboxes = det_res[0].boxes
            cropped_images = crop_images(uploaded_image, bboxes)
            
            # Clasificar cada detecci√≥n
            detections = []
            classes = []
            
            with torch.no_grad():
                for cropped_image in cropped_images:
                    resized_crop = cropped_image.resize(target_size)
                    inputs = processor(images=[resized_crop], return_tensors="pt")
                    outputs = clf_model(**inputs)
                    class_idx = outputs.logits.argmax(-1).item()
                    classes.append(class_idx)
                    
                    # Procesar atenciones para la detecci√≥n
                    attentions = outputs.attentions[-1][0]
                    attention_map = process_attention_maps(attentions, cropped_image)
                    
                    detections.append({
                        'original_image': cropped_image,
                        'attention_map': attention_map,
                        'class': CLASSES_NAME_ES[class_idx]
                    })
            
            # Guardar resultados en cach√©
            cached_results = {
                'full_image_classification': full_image_detection,
                'full_image_class': full_image_class,
                'detections': detections,
                'boxes': bboxes,
                'classes': classes
            }
            st.session_state.classification_cache[image_name] = cached_results
        
        # Almacenar los resultados en la lista temporal seg√∫n el modo actual
        processed_results.append({
            'image': uploaded_image,
            'filename': image_name,
            'boxes': bboxes if st.session_state.detection_toggle else [],
            'classes': classes if st.session_state.detection_toggle else [cached_results['full_image_class']],
            'detections': detections if st.session_state.detection_toggle else [cached_results['full_image_classification']]
        })
    
    # Actualizar el estado de la sesi√≥n con todos los resultados procesados
    st.session_state.processed_images = processed_results

def process_attention_maps(attentions, image):
    """
    Procesa los mapas de atenci√≥n de manera eficiente.
    """
    attention_maps_interpolated = []
    
    # Calcular importancia de cabezas y obtener las 3 principales
    cls_attention = attentions[:, 0, 1:]
    head_importance = cls_attention.mean(dim=1)
    top_heads = torch.argsort(head_importance, descending=True)[:3]
    
    # Normalizar los pesos de las cabezas
    head_weights = head_importance[top_heads]
    head_weights = head_weights / head_weights.sum()

    # Procesar cada una de las 3 cabezas principales
    for head_idx, weight in zip(top_heads, head_weights):
        attention = attentions[head_idx][0, 1:]
        grid_size = int(np.sqrt(attention.shape[0]))
        attention = attention.reshape(grid_size, grid_size)
        
        # Redimensionar el mapa de atenci√≥n
        attention = interpolate(
            attention.unsqueeze(0).unsqueeze(0),
            size=image.size[::-1],
            mode="bicubic",
            align_corners=False
        ).squeeze()
        
        # Normalizar y aplicar umbral
        attention = (attention - attention.min()) / (attention.max() - attention.min() + 1e-8)
        threshold = torch.quantile(attention, 0.7)
        attention_binary = (attention > threshold).float() * weight.item()
        
        attention_maps_interpolated.append(attention_binary.numpy())

    # Crear mapa combinado ponderado
    combined_attention = np.sum(attention_maps_interpolated, axis=0) if attention_maps_interpolated else np.zeros(image.size[::-1])
    return np.clip(combined_attention, 0, 1)

def export_results(processed_images: List[Dict[str, Any]]) -> None:
    """
    Exporta las im√°genes procesadas y sus anotaciones en un archivo ZIP.
    
    Args:
        processed_images: Lista de diccionarios con las im√°genes procesadas y sus metadatos
    """
    # Crear un archivo ZIP en memoria
    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, 'w') as zip_file:
        # Agregar cada imagen procesada al ZIP
        for processed in processed_images:
            # Crear figura con matplotlib para la visualizaci√≥n
            fig, ax = plt.subplots(1, 1, figsize=(10, 10))
            
            # Mostrar la imagen original
            img_array = np.array(processed['image'])
            ax.imshow(img_array)
            
            # Superponer el mapa de atenci√≥n
            for detection in processed['detections']:
                mask = np.ma.masked_where(detection['attention_map'] == 0, detection['attention_map'])
                ax.imshow(mask, cmap='Reds', alpha=0.6, vmin=0, vmax=1)
            
            ax.axis('off')
            plt.tight_layout(pad=0)
            
            # Convertir la figura a bytes
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, dpi=100)
            plt.close(fig)
            buf.seek(0)
            
            # Guardar en el ZIP con un nombre descriptivo
            filename = processed['filename']
            base_name = Path(filename).stem
            extension = Path(filename).suffix
            output_name = f"{base_name}_processed{extension}"
            zip_file.writestr(output_name, buf.getvalue())

        # Agregar el archivo CSV con las anotaciones
        zip_file.writestr('anotaciones.csv', write_csv(processed_images, CLASSES_NAME))

    # Preparar el archivo ZIP para la descarga
    zip_buffer.seek(0)
    zip_data = zip_buffer.getvalue()

    # Mostrar el bot√≥n de descarga
    try:
        st.sidebar.download_button(
            use_container_width=True,
            help='Exportar im√°genes procesadas y anotaciones',
            label="üì• Exportar",
            data=zip_data,
            file_name="upd.zip",
            mime="application/zip"
        )
    except Exception as ex:
        st.error("¬°No se ha subido ninguna imagen a√∫n!")
        st.error(ex)

def image_to_base64(image_path):
    """
    Convierte una imagen a formato base64 para mostrarla en la interfaz.
    
    Args:
        image_path: Ruta a la imagen
        
    Returns:
        String en formato base64 de la imagen
    """
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode()
    except FileNotFoundError:
        st.error(f"No se encontr√≥ la imagen en: {image_path}")
        return ""

def create_visualization(detection):
    """
    Crea la visualizaci√≥n de manera m√°s eficiente usando matplotlib.
    """
    fig, ax = plt.subplots(1, 1, figsize=(10, 10))
    
    # Mostrar la imagen original
    img_array = np.array(detection['original_image'])
    ax.imshow(img_array)
    
    # Superponer el mapa de atenci√≥n solo si show_attention es True y estamos en modo detecci√≥n
    if st.session_state.use_detection and st.session_state.show_attention:
        mask = np.ma.masked_where(detection['attention_map'] == 0, detection['attention_map'])
        ax.imshow(mask, cmap='Reds', alpha=0.6, vmin=0, vmax=1)
    
    ax.axis('off')
    plt.tight_layout(pad=0)
    
    # Convertir la figura a imagen PIL de manera m√°s eficiente
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, dpi=100)
    plt.close(fig)
    buf.seek(0)
    
    return PIL.Image.open(buf)

def main():
    """
    Funci√≥n principal que ejecuta la aplicaci√≥n Streamlit.
    Configura la interfaz y maneja la l√≥gica principal de la aplicaci√≥n.
    """
    # Configuraci√≥n de la p√°gina - DEBE ser el primer comando de Streamlit
    image_path = Path(__file__).parent / "huellas-humanas.png"
    st.set_page_config(
        page_title="UPD - √ölceras de Pie Diab√©tico",
        page_icon=str(image_path),
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Inicializar el estado de la sesi√≥n y variables
    initialize_session()
    source_imgs = []
    
    # Cargar los modelos
    try:
        det_model, processor, clf_model = load_models()
    except Exception as ex:
        st.error("No se pudo cargar los modelos. Verifique las rutas especificadas")
        st.error(ex)
        return
    
    # Mostrar logo y t√≠tulo
    try:
        base64_logo = image_to_base64(image_path)
        
        # Crear dos columnas para el logo y el t√≠tulo
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.markdown(
                f"""
            <style>
            .logo-title {{
                display: flex;
                align-items: center;
                gap: 15px;
            }}
            /* Eliminar padding del encabezado de la barra lateral */
            .st-emotion-cache-16idsys p {{
                padding-top: 0;
                padding-bottom: 0;
                margin: 0;
            }}
            /* Ajustar padding del contenedor principal */
            div[data-testid="stMainBlockContainer"] {{
                padding: 4rem 1rem 10rem 3rem 3rem;
            }}
            .st-emotion-cache-1jicfl2 {{
                padding-left: 3rem;
                padding-right: 3rem;
            }}
            .st-emotion-cache-kgpedg {{
                padding: 0 !important;
            }}
            </style>
            
            <div class="logo-title">
                <img src="data:image/png;base64,{base64_logo}" width="80">
                <h2>Cuidado inteligente del pie diab√©tico</h2>
            </div>
            <br>
                """,unsafe_allow_html=True
            )
    except Exception as e:
        st.error(f"Error al cargar la imagen: {e}")
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("<h2>Cuidado inteligente del pie diab√©tico</h2>", unsafe_allow_html=True)
    
    # Configuraci√≥n del sidebar
    with st.sidebar:
        # T√≠tulo de la barra lateral
        st.header("‚öôÔ∏è Configuraci√≥n del modelo")

        # Agregar un callback para el toggle de detecci√≥n
        def on_detection_toggle():
            """
            Callback que se ejecuta cuando cambia el estado del toggle de detecci√≥n
            """
            # Si se desactiva la detecci√≥n, forzar show_attention a True
            if not st.session_state.detection_toggle:
                st.session_state.show_attention = True
            else:
                # Si se reactiva la detecci√≥n, tambi√©n forzar show_attention a True
                st.session_state.show_attention = True
            
            # Reprocesar las im√°genes si hay alguna cargada
            if 'uploaded_images' in st.session_state and st.session_state.uploaded_images:
                process_images(det_model, clf_model, processor, st.session_state.confidence/100, IOU_THRES, CLASSES_NAME)

        # Toggle para elegir el modo de procesamiento
        st.toggle(
            "Usar modelo de detecci√≥n",
            value=True,
            key="detection_toggle",
            on_change=on_detection_toggle,
            help="Activar/Desactivar modelo de detecci√≥n de UPD"
        )

        # Mostrar el toggle de atenci√≥n solo si la detecci√≥n est√° activada
        if st.session_state.detection_toggle:
            def on_attention_toggle():
                """
                Callback que se ejecuta cuando cambia el estado del toggle de atenci√≥n
                """
                # Reprocesar las im√°genes para actualizar la visualizaci√≥n
                if 'uploaded_images' in st.session_state and st.session_state.uploaded_images:
                    process_images(det_model, clf_model, processor, st.session_state.confidence/100, IOU_THRES, CLASSES_NAME)

            # Inicializar show_attention en el session state si no existe
            if 'show_attention' not in st.session_state:
                st.session_state.show_attention = True

            st.toggle(
                "Mostrar zonas de atenci√≥n",
                key="show_attention",
                on_change=on_attention_toggle,
                help="Activar/Desactivar la visualizaci√≥n de las zonas de atenci√≥n"
            )
        else:
            # Si la detecci√≥n est√° desactivada, forzar show_attention a True
            st.session_state.show_attention = True

        # Control deslizante para ajustar la confianza del modelo
        confidence = st.slider( 
            label="üìç Seleccionar confianza de detecci√≥n",
            min_value=0,
            max_value=100, 
            value=st.session_state.confidence,
            help='Probabilidad de certeza en la detecci√≥n de la √∫lcera',
            disabled=not st.session_state.use_detection,
            key="confidence_slider"
        )

        # Actualizar la confianza solo si el slider est√° habilitado
        if st.session_state.use_detection and confidence != st.session_state.confidence:
            st.session_state.confidence = confidence
            clear_session()

        # Contenedor para el cargador de archivos
        uploader_container = st.container()
        with uploader_container:
            source_imgs = st.file_uploader(
                label="üìç Seleccionar im√°genes",
                help='Imagen del pie que desea analizar',
                type=("jpg", "jpeg", "png"),
                accept_multiple_files=True,
                key=f"image_uploader_{st.session_state.uploader_key}"
            )

        # Personalizar el estilo del cargador
        style_language_uploader()

        # Verificar l√≠mite de im√°genes
        if source_imgs and len(source_imgs) > 20:
            st.toast('‚ö†Ô∏è Ha superado el l√≠mite de im√°genes')
            time.sleep(.5)        

        # Verificar duplicados
        if source_imgs:
            duplicate_files = check_duplicates(source_imgs)
            if duplicate_files:
                if len(duplicate_files) == 1:
                    st.error(f"El siguiente archivo est√° duplicado: {duplicate_files[0]}")
                else:
                    st.error(f"‚ö†Ô∏è Los siguientes archivos est√°n duplicados: \n{', '.join(duplicate_files)}")
            
        # Botones de control
        if source_imgs and len(source_imgs) != 0:
            delete_button = st.button(
                "üóëÔ∏è Eliminar im√°genes",
                use_container_width=True,
                help="Eliminar todas las im√°genes cargadas",
                key="delete_button"
            )

            # Bot√≥n de an√°lisis y procesamiento de im√°genes
            if len(source_imgs) <= 20 and not check_duplicates(source_imgs):
                text_btn = 'üîç Analizar im√°genes' if len(source_imgs) > 1 else 'üîç Analizar imagen'
                process_image_button = st.button(
                    label=text_btn,
                    help="Analizar las im√°genes seleccionadas",
                    use_container_width=True,
                    key="process_button"
                )
            else:
                process_image_button = False
        else:
            delete_button = False
            process_image_button = False

    # Mostrar mensaje cuando no hay im√°genes
    if not source_imgs or len(source_imgs) == 0:
        camera_svg = '''
            <svg xmlns="http://www.w3.org/2000/svg" fill="gray" viewBox="0 0 24 24" width="24" height="24">
                <circle cx="16" cy="8.011" r="2.5"/>
                <path d="M23,16a1,1,0,0,0-1,1v2a3,3,0,0,1-3,3H17a1,1,0,0,0,0,2h2a5.006,5.006,0,0,0,5-5V17A1,1,0,0,0,23,16Z"/>
                <path d="M1,8A1,1,0,0,0,2,7V5A3,3,0,0,1,5,2H7A1,1,0,0,0,7,0H5A5.006,5.006,0,0,0,0,5V7A1,1,0,0,0,1,8Z"/>
                <path d="M7,22H5a3,3,0,0,1-3-3V17a1,1,0,0,0-2,0v2a5.006,5.006,0,0,0,5,5H7a1,1,0,0,0,0-2Z"/>
                <path d="M19,0H17a1,1,0,0,0,0,2h2a3,3,0,0,1,3,3V7a1,1,0,0,0,2,0V5A5.006,5.006,0,0,0,19,0Z"/>
                <path d="M18.707,17.293,11.121,9.707a3,3,0,0,0-4.242,0L4.586,12A2,2,0,0,0,4,13.414V16a3,3,0,0,0,3,3H18a1,1,0,0,0,.707-1.707Z"/>
            </svg>'''

        with st.container(border=True):
            st.markdown(
                f"<div style=' font-size: 16px; display: flex; justify-content: center; align-items: center; padding: 0 0 10px 0; gap: 15px; border-radius: 8px;'>"
                f"{camera_svg}"
                "No ha seleccionado una imagen para su procesamiento"
                "</div>",
                unsafe_allow_html=True
            )

    # Procesar y mostrar las im√°genes
    if source_imgs and len(source_imgs) != 0:
        st.session_state.uploaded_images = source_imgs

        # Lista de nombres de im√°genes procesadas
        processed_filenames = [img['filename'] for img in st.session_state.processed_images] if st.session_state.processed_images else []
        uploaded_filenames = [img.name for img in st.session_state.uploaded_images]

        # Verificar si hay nuevas im√°genes que procesar
        new_images = [name for name in uploaded_filenames if name not in processed_filenames]

        # Selector de imagen para visualizaci√≥n
        if len(st.session_state.uploaded_images) > 1:
            image_filenames = [img.name for img in st.session_state.uploaded_images]
            
            # Inicializar o actualizar el estado de la imagen seleccionada
            if 'selected_image_name' not in st.session_state or st.session_state.selected_image_name not in image_filenames:
                # Si hay nuevas im√°genes, seleccionar la primera de ellas
                if new_images:
                    st.session_state.selected_image_name = new_images[0]
                else:
                    st.session_state.selected_image_name = image_filenames[0]

            # Usar el estado de la sesi√≥n para mantener la selecci√≥n
            selected_image = st.selectbox(
                "",
                image_filenames,
                index=image_filenames.index(st.session_state.selected_image_name),
                help="Selecciona la imagen que desea visualizar",
                key="image_selector",
                on_change=lambda: setattr(st.session_state, 'selected_image_name', st.session_state.image_selector)
            )
            
            # Actualizar el estado con la imagen seleccionada
            st.session_state.selected_image_name = selected_image
            
            # Encontrar la imagen correspondiente
            for img in source_imgs:
                if img.name == selected_image:
                    source_img = img
                    break
            else:
                source_img = source_imgs[0]  # Fallback a la primera imagen si no se encuentra
        else:
            selected_image = source_imgs[0].name
            source_img = source_imgs[0]

        # Mostrar im√°genes en columnas
        col1, col2 = st.columns([1,1], gap="medium")
        with col1:
            try:
                st.markdown("""
                    <div style='text-align: center; font-size: 1.5em;'>
                        üì∏ Imagen Original
                    </div>
                """, unsafe_allow_html=True)
                with st.container():
                    st.markdown("""
                        <style>
                            .stImage img {
                                max-height: 400px !important;
                                height: auto !important;
                                max-width: 400px !important;
                                object-fit: contain !important;
                                overflow: hidden;
                                margin: 0 auto !important;
                                display: block !important;
                            }
                            .st-emotion-cache-1kyxreq {
                                width: 100% !important;
                                margin: 0 !important;
                                padding: 0 !important;
                                display: flex !important;
                                justify-content: center !important;
                                align-items: center !important;
                            }
                            .st-emotion-cache-1v0mbdj {
                                width: 100% !important;
                                margin: 0 !important;
                                padding: 0 !important;
                                display: flex !important;
                                justify-content: center !important;
                                align-items: center !important;
                            }
                            div[data-testid="stImage"] {
                                display: flex !important;
                                justify-content: center !important;
                                align-items: center !important;
                                width: 100% !important;
                            }
                        </style>
                    """, unsafe_allow_html=True)
                    st.image(source_img, use_column_width=True)
            except Exception as ex:
                st.error("Ocurri√≥ un error al abrir la imagen.")
                st.error(ex)

        # Procesar y mostrar resultados
        with col2:
            if delete_button:
                clear_session()
                st.toast('Todas las im√°genes han sido eliminadas', icon='‚úÖ')
                time.sleep(0.6)
                st.rerun()

            # Lista de nombres de im√°genes procesadas
            processed_filenames = [img['filename'] for img in st.session_state.processed_images] if st.session_state.processed_images else []
            uploaded_filenames = [img.name for img in st.session_state.uploaded_images]

            # Verificar si hay nuevas im√°genes que procesar
            new_images = [name for name in uploaded_filenames if name not in processed_filenames]
            
            # Procesar autom√°ticamente solo si hay im√°genes procesadas previamente y hay nuevas im√°genes
            if st.session_state.processed_images and new_images:
                process_images(
                    det_model=det_model,
                    clf_model=clf_model,
                    processor=processor,
                    confidence=st.session_state.confidence/100,
                    iou_thres=IOU_THRES,
                    classes_name=CLASSES_NAME_ES
                )
            # Si se presion√≥ el bot√≥n de procesar, procesar todas las im√°genes
            elif process_image_button:
                process_images(
                    det_model=det_model,
                    clf_model=clf_model,
                    processor=processor,
                    confidence=st.session_state.confidence/100,
                    iou_thres=IOU_THRES,
                    classes_name=CLASSES_NAME_ES
                )

            # Mostrar im√°genes procesadas
            if st.session_state.processed_images:
                for processed in st.session_state.processed_images:
                    if processed['filename'] == selected_image:
                        if len(processed['detections']) > 0:
                            title = "üîç √ölceras Detectadas"
                            st.markdown(f"""
                                <div style='text-align: center; font-size: 1.5em; margin-bottom: 0.5rem;'>
                                    {title}
                                </div>
                            """, unsafe_allow_html=True)
                            with st.container():
                                # Mostrar cada detecci√≥n verticalmente
                                for idx, detection in enumerate(processed['detections']):
                                    # Crear figura con matplotlib para la visualizaci√≥n
                                    fig, ax = plt.subplots(1, 1, figsize=(10, 10))
                                    # Mostrar la imagen original
                                    img_array = np.array(detection['original_image'])
                                    ax.imshow(img_array)
                                    # Superponer el mapa de atenci√≥n solo si show_attention es True
                                    if st.session_state.show_attention:
                                        mask = np.ma.masked_where(detection['attention_map'] == 0, detection['attention_map'])
                                        ax.imshow(mask, cmap='Reds', alpha=0.6, vmin=0, vmax=1)
                                    ax.axis('off')
                                    plt.tight_layout(pad=0)
                                    buf = io.BytesIO()
                                    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, dpi=100)
                                    plt.close(fig)
                                    buf.seek(0)
                                    display_image = PIL.Image.open(buf)
                                    st.image(
                                        display_image,
                                        use_column_width=True
                                    )
                                    st.markdown(
                                        f"""
                                        <div style="
                                            background-color: #FF4B4B;
                                            color: white;
                                            padding: 8px 12px;
                                            border-radius: 6px;
                                            text-align: center;
                                            margin: 5px auto 15px auto;
                                            font-weight: 600;
                                            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                                            width: 80%;
                                            box-sizing: border-box;
                                        ">
                                            {f'Detecci√≥n {idx + 1}: ' if st.session_state.use_detection else ''}{detection['class']}
                                        </div>
                                        """,
                                        unsafe_allow_html=True
                                    )
                        else:
                            st.markdown("""
                                <div style="
                                    background-color: transparent;
                                    padding: 2rem;
                                    text-align: center;
                                    margin: 1rem 0;
                                ">
                                    <div style="
                                        width: 100px;
                                        height: 100px;
                                        background-color: #00C853;
                                        border-radius: 50%;
                                        display: flex;
                                        align-items: center;
                                        justify-content: center;
                                        margin: 3rem auto 1rem auto;
                                        box-shadow: 0 4px 8px rgba(0,200,83,0.3);
                                    ">
                                        <div style="
                                            font-size: 4rem;
                                            color: white;
                                            transform: scale(1.2);
                                        ">
                                            ‚úì
                                        </div>
                                    </div>
                                    <div style="
                                        font-size: 1.2rem;
                                        color: var(--text-color);
                                        font-weight: 800;
                                    ">
                                        No se han detectado ulceraciones
                                    </div>
                                    <div style="
                                        font-size: 0.9rem;
                                        color: var(--text-color);
                                        margin-top: 0.5rem;
                                        opacity: 0.8;
                                    ">
                                        La imagen analizada no presenta signos de √∫lceras
                                    </div>
                                </div>
                                <style>
                                    :root {
                                        --text-color: var(--text-color);
                                    }
                                    @media (prefers-color-scheme: dark) {
                                        :root {
                                            --text-color: #FFFFFF;
                                        }
                                    }
                                    @media (prefers-color-scheme: light) {
                                        :root {
                                            --text-color: #000000;
                                        }
                                    }
                                </style>
                            """, unsafe_allow_html=True)

            # Mostrar bot√≥n de exportaci√≥n si hay im√°genes procesadas
            if st.session_state.processed_images and len(st.session_state.processed_images) == len(st.session_state.uploaded_images):
                # Mostrar el bot√≥n de exportaci√≥n en ambos modos
                export_results(st.session_state.processed_images)

def update_detection_mode():
    """
    Actualiza el modo de detecci√≥n y la visualizaci√≥n de las im√°genes procesadas.
    """
    # Actualizar la visualizaci√≥n de las im√°genes procesadas si existen
    if 'processed_images' in st.session_state and len(st.session_state.processed_images) > 0:
        for processed in st.session_state.processed_images:
            image_name = processed['filename']
            if image_name in st.session_state.classification_cache:
                cached_results = st.session_state.classification_cache[image_name]
                if st.session_state.use_detection:
                    # Si estamos en modo detecci√≥n, usar los resultados de detecci√≥n
                    processed['detections'] = cached_results['detections']
                    processed['boxes'] = cached_results['boxes']
                    processed['classes'] = cached_results['classes']
                else:
                    # Si estamos en modo clasificaci√≥n directa, usar la clasificaci√≥n de la imagen completa
                    processed['detections'] = [cached_results['full_image_classification']]
                    processed['boxes'] = []
                    processed['classes'] = [cached_results['full_image_class']]

if __name__ == '__main__':
    main()